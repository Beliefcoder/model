#!/usr/bin/env python
# -*- coding: utf-8 -*-

import torch
import numpy as np
from nltk.translate.bleu_score import sentence_bleu, corpus_bleu
from nltk.translate.meteor_score import meteor_score
import nltk
import json
import os

class TranslationEvaluator:
    """ç¿»è¯‘è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        # ç¡®ä¿nltkæ•°æ®å·²ä¸‹è½½
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
    
    def calculate_bleu(self, references, hypotheses):
        """è®¡ç®—BLEUåˆ†æ•°"""
        # å•ä¸ªå¥å­BLEU
        sentence_bleu_scores = []
        for ref, hyp in zip(references, hypotheses):
            ref_tokens = [ref.split()]
            hyp_tokens = hyp.split()
            score = sentence_bleu(ref_tokens, hyp_tokens)
            sentence_bleu_scores.append(score)
        
        # è¯­æ–™åº“BLEU
        ref_corpus = [[ref.split()] for ref in references]
        hyp_corpus = [hyp.split() for hyp in hypotheses]
        corpus_bleu_score = corpus_bleu(ref_corpus, hyp_corpus)
        
        return {
            "sentence_bleu_mean": np.mean(sentence_bleu_scores),
            "sentence_bleu_std": np.std(sentence_bleu_scores),
            "corpus_bleu": corpus_bleu_score,
            "bleu_1": corpus_bleu(ref_corpus, hyp_corpus, weights=(1, 0, 0, 0)),
            "bleu_2": corpus_bleu(ref_corpus, hyp_corpus, weights=(0.5, 0.5, 0, 0)),
            "bleu_3": corpus_bleu(ref_corpus, hyp_corpus, weights=(0.33, 0.33, 0.33, 0)),
            "bleu_4": corpus_bleu(ref_corpus, hyp_corpus, weights=(0.25, 0.25, 0.25, 0.25))
        }
    
    def calculate_meteor(self, references, hypotheses):
        """è®¡ç®—METEORåˆ†æ•°"""
        meteor_scores = []
        for ref, hyp in zip(references, hypotheses):
            score = meteor_score([ref], hyp)
            meteor_scores.append(score)
        
        return {
            "meteor_mean": np.mean(meteor_scores),
            "meteor_std": np.std(meteor_scores)
        }
    
    def calculate_rouge(self, references, hypotheses):
        """è®¡ç®—ROUGEåˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œå®ç°ROUGE-Lçš„ç®€åŒ–ç‰ˆæœ¬
        rouge_l_scores = []
        
        for ref, hyp in zip(references, hypotheses):
            ref_words = ref.split()
            hyp_words = hyp.split()
            
            # è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—
            lcs_length = self.longest_common_subsequence(ref_words, hyp_words)
            
            if len(ref_words) == 0 or len(hyp_words) == 0:
                rouge_l_scores.append(0.0)
                continue
            
            precision = lcs_length / len(hyp_words)
            recall = lcs_length / len(ref_words)
            
            if precision + recall == 0:
                f1 = 0.0
            else:
                f1 = 2 * precision * recall / (precision + recall)
            
            rouge_l_scores.append(f1)
        
        return {
            "rouge_l_mean": np.mean(rouge_l_scores),
            "rouge_l_std": np.std(rouge_l_scores)
        }
    
    def longest_common_subsequence(self, seq1, seq2):
        """è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦"""
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        return dp[m][n]
    
    def calculate_perplexity(self, model, data_loader, device):
        """è®¡ç®—å›°æƒ‘åº¦"""
        model.eval()
        total_loss = 0
        total_tokens = 0
        
        with torch.no_grad():
            for batch in data_loader:
                src = batch['src_ids'].to(device)
                tgt = batch['tgt_ids'].to(device)
                
                tgt_input = tgt[:, :-1]
                tgt_labels = tgt[:, 1:]
                
                logits = model(src, tgt_input)
                loss = torch.nn.functional.cross_entropy(
                    logits.reshape(-1, logits.size(-1)), 
                    tgt_labels.reshape(-1),
                    reduction='sum'
                )
                
                total_loss += loss.item()
                total_tokens += (tgt_labels != 0).sum().item()  # å¿½ç•¥padding
        
        avg_loss = total_loss / total_tokens
        perplexity = torch.exp(torch.tensor(avg_loss)).item()
        
        return perplexity
    
    def calculate_accuracy(self, model, data_loader, device):
        """è®¡ç®—å‡†ç¡®ç‡"""
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for batch in data_loader:
                src = batch['src_ids'].to(device)
                tgt = batch['tgt_ids'].to(device)
                
                tgt_input = tgt[:, :-1]
                tgt_labels = tgt[:, 1:]
                
                logits = model(src, tgt_input)
                predictions = logits.argmax(dim=-1)
                
                # åªè®¡ç®—épaddingä½ç½®çš„å‡†ç¡®ç‡
                mask = (tgt_labels != 0)
                correct += ((predictions == tgt_labels) & mask).sum().item()
                total += mask.sum().item()
        
        accuracy = correct / total if total > 0 else 0
        return accuracy

def comprehensive_evaluation(model, data_loader, tokenizer, device, num_samples=100):
    """ç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    evaluator = TranslationEvaluator(tokenizer)
    
    print("å¼€å§‹ç»¼åˆè¯„ä¼°...")
    
    # 1. è®¡ç®—å›°æƒ‘åº¦
    perplexity = evaluator.calculate_perplexity(model, data_loader, device)
    print(f"å›°æƒ‘åº¦: {perplexity:.2f}")
    
    # 2. è®¡ç®—å‡†ç¡®ç‡
    accuracy = evaluator.calculate_accuracy(model, data_loader, device)
    print(f"å‡†ç¡®ç‡: {accuracy:.3f}")
    
    # 3. ç”Ÿæˆç¿»è¯‘å¹¶è®¡ç®—å…¶ä»–æŒ‡æ ‡
    references = []
    hypotheses = []
    
    model.eval()
    with torch.no_grad():
        for i, batch in enumerate(data_loader):
            if i >= num_samples:  # é™åˆ¶æ ·æœ¬æ•°é‡
                break
                
            src = batch['src_ids'].to(device)
            src_texts = batch['src_texts']
            tgt_texts = batch['tgt_texts']
            
            # ä½¿ç”¨è´ªå¿ƒæœç´¢ç”Ÿæˆç¿»è¯‘
            generated_translations = greedy_decode_batch(model, src, tokenizer, device)
            
            references.extend(tgt_texts)
            hypotheses.extend(generated_translations)
    
    # 4. è®¡ç®—BLEUåˆ†æ•°
    bleu_metrics = evaluator.calculate_bleu(references, hypotheses)
    print(f"è¯­æ–™åº“BLEU: {bleu_metrics['corpus_bleu']:.3f}")
    print(f"BLEU-1: {bleu_metrics['bleu_1']:.3f}, BLEU-2: {bleu_metrics['bleu_2']:.3f}")
    print(f"BLEU-3: {bleu_metrics['bleu_3']:.3f}, BLEU-4: {bleu_metrics['bleu_4']:.3f}")
    
    # 5. è®¡ç®—METEORåˆ†æ•°
    meteor_metrics = evaluator.calculate_meteor(references, hypotheses)
    print(f"METEOR: {meteor_metrics['meteor_mean']:.3f}")
    
    # 6. è®¡ç®—ROUGEåˆ†æ•°
    rouge_metrics = evaluator.calculate_rouge(references, hypotheses)
    print(f"ROUGE-L: {rouge_metrics['rouge_l_mean']:.3f}")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    results = {
        "perplexity": perplexity,
        "accuracy": accuracy,
        "bleu_metrics": bleu_metrics,
        "meteor_metrics": meteor_metrics,
        "rouge_metrics": rouge_metrics,
        "sample_translations": [
            {
                "source": src,
                "reference": ref,
                "hypothesis": hyp
            }
            for src, ref, hyp in zip(src_texts[:5], references[:5], hypotheses[:5])
        ]
    }
    
    # ä¿å­˜ç»“æœ
    os.makedirs("results", exist_ok=True)
    with open("results/comprehensive_evaluation.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    return results

def greedy_decode_batch(model, src, tokenizer, device, max_length=64):
    """æ‰¹é‡è´ªå¿ƒè§£ç """
    batch_size = src.size(0)
    translations = []
    
    for i in range(batch_size):
        src_single = src[i:i+1]
        translation = greedy_decode_single(model, src_single, tokenizer, device, max_length)
        translations.append(translation)
    
    return translations

def greedy_decode_single(model, src, tokenizer, device, max_length=64):
    """å•ä¸ªå¥å­è´ªå¿ƒè§£ç """
    model.eval()
    
    # ç¼–ç å™¨å‰å‘ä¼ æ’­
    with torch.no_grad():
        src_emb = model.encoder_embedding(src)
        src_emb = model.encoder_pos(src_emb)
        enc_output = model.encoder(src_emb)
    
    # åˆå§‹åŒ–è§£ç å™¨è¾“å…¥
    tgt = torch.tensor([[tokenizer.bos_token_id]], device=device)
    
    for i in range(max_length):
        with torch.no_grad():
            tgt_emb = model.decoder_embedding(tgt)
            tgt_emb = model.decoder_pos(tgt_emb)
            dec_output = model.decoder(tgt_emb, enc_output)
            logits = model.fc(dec_output[:, -1, :])
            next_token = logits.argmax(dim=-1, keepdim=True)
        
        tgt = torch.cat([tgt, next_token], dim=1)
        
        # å¦‚æœç”Ÿæˆäº†EOS tokenï¼Œåœæ­¢ç”Ÿæˆ
        if next_token.item() == tokenizer.eos_token_id:
            break
    
    # è§£ç ä¸ºæ–‡æœ¬
    translation = tokenizer.decode(tgt.squeeze().tolist())
    return translation

#def main():
#    """è¿è¡Œç»¼åˆè¯„ä¼°"""
#    import sys
#    sys.path.append(os.path.dirname(os.path.dirname(__file__)))
#    
#    from config.training_config import TrainingConfig
#    from data.tokenizer import build_tokenizer
#    from data.dataset_loader import get_data_loaders
#    from model.light_transformer import LightTransformer
#    
#    # åŠ è½½é…ç½®å’Œæ¨¡å‹
#    config = TrainingConfig()
#    tokenizer = build_tokenizer(config)
#    
#    # åŠ è½½æ•°æ®
#    _, val_loader = get_data_loaders(config, tokenizer)
#    
#    # åŠ è½½æ¨¡å‹
#    device = torch.device(config.DEVICE)
#    model = LightTransformer(config).to(device)
#    
#    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
#    checkpoint_path = "outputs/checkpoints/latest_transformer.pth"
#    if os.path.exists(checkpoint_path):
#        model.load_state_dict(torch.load(checkpoint_path, map_location=device))
#        print(f"åŠ è½½æ¨¡å‹æƒé‡: {checkpoint_path}")
#    else:
#        print("è­¦å‘Š: æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
#    
#    # è¿è¡Œç»¼åˆè¯„ä¼°
#    results = comprehensive_evaluation(model, val_loader, tokenizer, device)
#    
#    print("\nç»¼åˆè¯„ä¼°å®Œæˆ!")
#    print(f"ä¸»è¦æŒ‡æ ‡:")
#    print(f"  å›°æƒ‘åº¦: {results['perplexity']:.2f}")
#    print(f"  å‡†ç¡®ç‡: {results['accuracy']:.3f}")
#    print(f"  BLEU-4: {results['bleu_metrics']['bleu_4']:.3f}")
#    print(f"  METEOR: {results['meteor_metrics']['meteor_mean']:.3f}")
#    print(f"  ROUGE-L: {results['rouge_metrics']['rouge_l_mean']:.3f}")
def main():
    """è¿è¡Œç»¼åˆè¯„ä¼°ï¼ˆå¼ºåˆ¶åŒ¹é…è®­ç»ƒæ—¶çš„é…ç½®ï¼‰"""
    import sys
    sys.path.append(os.path.dirname(os.path.dirname(__file__)))
    
    # -------------------------- å…³é”®1ï¼šåŒ¹é…è®­ç»ƒæ—¶çš„ Config é…ç½® --------------------------
    from config.training_config import TrainingConfig  # è‹¥è®­ç»ƒæ—¶ç”¨äº†å…¶ä»–Configï¼Œæ›¿æ¢æˆå¯¹åº”çš„
    config = TrainingConfig()
    
    # å¼ºåˆ¶è®¾ç½®ï¼šå’Œè®­ç»ƒæ—¶çš„æƒé‡å½¢çŠ¶åŒ¹é…ï¼ˆVOCAB_SIZE=36ï¼‰
    config.VOCAB_SIZE = 36  # å…³é”®ï¼æ‰‹åŠ¨æ”¹æˆè®­ç»ƒæ—¶çš„è¯æ±‡è¡¨å¤§å°ï¼ˆ36ï¼‰
    config.USE_LOCAL_DATA = False  # å¼ºåˆ¶ç¦ç”¨æœ¬åœ°æ•°æ®ï¼Œç¨³å®šåŠ è½½å…¬å¼€æ•°æ®é›†
    config.MAX_LENGTH = 64  # ä¿æŒå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼ˆè‹¥è®­ç»ƒæ—¶æ”¹äº†ï¼Œè¿™é‡ŒåŒæ­¥æ”¹ï¼‰
    
    # -------------------------- å…³é”®2ï¼šåŠ è½½ä¾èµ–ç»„ä»¶ --------------------------
    from data.tokenizer import build_tokenizer
    from data.dataset_loader import get_data_loaders
    from model.light_transformer import LightTransformer
    
    # åŠ è½½åˆ†è¯å™¨ï¼ˆæ­¤æ—¶åˆ†è¯å™¨çš„è¯æ±‡è¡¨å¤§å°ä¼šå’Œ config.VOCAB_SIZE ä¸€è‡´ï¼‰
    tokenizer = build_tokenizer(config)
    
    # ç¨³å®šåŠ è½½å…¬å¼€æ•°æ®é›†ï¼ˆç¦ç”¨æœ¬åœ°æ•°æ®ï¼Œé¿å…æ‰¾ data/custom_datasetï¼‰
    _, val_loader = get_data_loaders(config, tokenizer)
    print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼šéªŒè¯é›†å¤§å° {len(val_loader.dataset)}")
    
    # -------------------------- å…³é”®3ï¼šåŠ è½½æ¨¡å‹ï¼ˆä¿®å¤æƒé‡åŒ¹é…+å®‰å…¨è­¦å‘Šï¼‰ --------------------------
    device = torch.device(config.DEVICE if torch.cuda.is_available() else "cpu")
    model = LightTransformer(config).to(device)  # æ­¤æ—¶æ¨¡å‹å‚æ•°å½¢çŠ¶æ˜¯ [36, 256]ï¼Œå’Œæƒé‡åŒ¹é…
    
    checkpoint_path = "outputs/checkpoints/latest_transformer.pth"
    if os.path.exists(checkpoint_path):
        # ä¿®å¤å®‰å…¨è­¦å‘Šï¼šæ·»åŠ  weights_only=Trueï¼›æƒé‡å½¢çŠ¶å·²åŒ¹é…ï¼Œèƒ½æ­£å¸¸åŠ è½½
        model.load_state_dict(torch.load(checkpoint_path, map_location=device, weights_only=True))
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡ï¼ˆè¯æ±‡è¡¨å¤§å° 36ï¼Œå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰")
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
    
    # -------------------------- è¿è¡Œè¯„ä¼° --------------------------
    results = comprehensive_evaluation(model, val_loader, tokenizer, device)
    
    print("\nğŸ‰ ç»¼åˆè¯„ä¼°å®Œæˆ!")
    print(f"ä¸»è¦æŒ‡æ ‡:")
    print(f"  å›°æƒ‘åº¦: {results['perplexity']:.2f}")
    print(f"  å‡†ç¡®ç‡: {results['accuracy']:.3f}")
    print(f"  BLEU-4: {results['bleu_metrics']['bleu_4']:.3f}")
    print(f"  METEOR: {results['meteor_metrics']['meteor_mean']:.3f}")
    print(f"  ROUGE-L: {results['rouge_metrics']['rouge_l_mean']:.3f}")
    print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: results/comprehensive_evaluation.json")

if __name__ == "__main__":
    main()