from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing
from tokenizers.normalizers import NFKC
import os
import json
import warnings

def build_tokenizer(config, dataset=None):
    """æ„å»ºåŸºäºBPEçš„åˆ†è¯å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼šå¼ºåˆ¶ç”¨çœŸå®æ•°æ®é›†è®­ç»ƒï¼Œè¯æ±‡è¡¨å¤§å°è¾¾æ ‡ï¼‰"""
    # 1. é…ç½®åˆ†è¯å™¨ä¿å­˜è·¯å¾„ï¼ˆä»configè¯»å–ï¼Œç¡®ä¿è®­ç»ƒ/è¯„ä¼°ä¸€è‡´ï¼‰
    tokenizer_path = getattr(config, "TOKENIZER_PATH", "outputs/tokenizer.json")
    os.makedirs(os.path.dirname(tokenizer_path), exist_ok=True)

    # 2. ä¼˜å…ˆåŠ è½½å·²è®­ç»ƒçš„åˆ†è¯å™¨ï¼ˆè‹¥å­˜åœ¨ä¸”è¯æ±‡è¡¨å¤§å°è¾¾æ ‡ï¼‰
    if os.path.exists(tokenizer_path):
        tokenizer = Tokenizer.from_file(tokenizer_path)
        vocab_size = tokenizer.get_vocab_size()
        # æ£€æŸ¥è¯æ±‡è¡¨å¤§å°æ˜¯å¦åˆç†ï¼ˆç¿»è¯‘ä»»åŠ¡è‡³å°‘8192ï¼‰
        if vocab_size >= 8192:
            print(f"âœ… åŠ è½½å·²æœ‰åˆ†è¯å™¨ï¼š{tokenizer_path}ï¼ˆè¯æ±‡è¡¨å¤§å°ï¼š{vocab_size}ï¼‰")
            return tokenizer
        else:
            warnings.warn(f"âš ï¸  å·²å­˜åœ¨çš„åˆ†è¯å™¨è¯æ±‡è¡¨è¿‡å°ï¼ˆ{vocab_size}ï¼‰ï¼Œå°†é‡æ–°è®­ç»ƒ")
            os.remove(tokenizer_path)  # åˆ é™¤å°è¯æ±‡è¡¨åˆ†è¯å™¨

    # 3. è®­ç»ƒæ–°åˆ†è¯å™¨ï¼šå¼ºåˆ¶è¦æ±‚ä¼ å…¥çœŸå®æ•°æ®é›†ï¼ˆç¦æ­¢ç”¨ç¤ºä¾‹æ–‡æœ¬ï¼‰
    print("ğŸ“¥ å¼€å§‹è®­ç»ƒæ–°åˆ†è¯å™¨...")
    if dataset is None:
        raise ValueError("âŒ è®­ç»ƒåˆ†è¯å™¨å¿…é¡»ä¼ å…¥çœŸå®æ•°æ®é›†ï¼è¯·åœ¨train.pyä¸­ä¼ å…¥æ•°æ®é›†ç»™build_tokenizer")

    # 4. åˆå§‹åŒ–åˆ†è¯å™¨ï¼ˆæ–°å¢å½’ä¸€åŒ–å™¨ï¼Œä¼˜åŒ–æ–‡æœ¬å¤„ç†ï¼‰
    tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
    tokenizer.normalizer = NFKC()  # ç»Ÿä¸€å­—ç¬¦æ ¼å¼ï¼ˆå¦‚å…¨è§’è½¬åŠè§’ã€Unicodeæ ‡å‡†åŒ–ï¼‰
    tokenizer.pre_tokenizer = Whitespace()  # æŒ‰ç©ºæ ¼åˆ†è¯ï¼ˆé€‚åˆè‹±/å¾·ç­‰è¯­è¨€ï¼Œä¸­æ–‡éœ€æ¢ç”¨Charé¢„åˆ†è¯ï¼‰

    # 5. é…ç½®BPEè®­ç»ƒå™¨ï¼ˆä¼˜åŒ–å‚æ•°ï¼‰
    trainer = BpeTrainer(
        vocab_size=config.VOCAB_SIZE,  # ä»configè¯»å–ï¼ˆå»ºè®®8192æˆ–16384ï¼‰
        special_tokens=["[PAD]", "[UNK]", "[BOS]", "[EOS]"],  # ç‰¹æ®Štokené¡ºåºå›ºå®š
        min_frequency=2,  # ä¿ç•™ï¼šè¿‡æ»¤å‡ºç°æ¬¡æ•°<2çš„ç¨€æœ‰è¯
        show_progress=True,  # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
    )

    # 6. å‡†å¤‡è®­ç»ƒè¯­æ–™ï¼ˆä»å¹³è¡Œè¯­æ–™ä¸­æå–æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€æ–‡æœ¬ï¼‰
    def get_training_corpus():
        batch_size = 1000  # æ‰¹é‡ç”Ÿæˆï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
        batch = []
        for example in dataset:
            # é€‚é…å¸¸è§å¹³è¡Œè¯­æ–™æ ¼å¼ï¼ˆæ ¹æ®ä½ çš„æ•°æ®é›†è°ƒæ•´å­—æ®µåï¼‰
            if 'translation' in example:
                # æ ¼å¼1ï¼šHugging Faceå…¬å¼€æ•°æ®é›†ï¼ˆå¦‚iwslt2017-en-deï¼‰
                src_text = example['translation'].get(config.SRC_LANG, "")  # æºè¯­è¨€ï¼ˆå¦‚enï¼‰
                tgt_text = example['translation'].get(config.TGT_LANG, "")  # ç›®æ ‡è¯­è¨€ï¼ˆå¦‚deï¼‰
            elif config.USE_LOCAL_DATA and 'source' in example and 'target' in example:
                # æ ¼å¼2ï¼šæœ¬åœ°å¹³è¡Œè¯­æ–™ï¼ˆsource/targetå­—æ®µï¼‰
                src_text = example['source']
                tgt_text = example['target']
            else:
                # æ ¼å¼3ï¼šè‡ªå®šä¹‰å­—æ®µï¼ˆå¦‚en/deï¼‰
                src_text = example.get(config.SRC_LANG, "")
                tgt_text = example.get(config.TGT_LANG, "")
            
            # è¿‡æ»¤ç©ºæ–‡æœ¬ï¼Œæ·»åŠ åˆ°è®­ç»ƒè¯­æ–™
            if src_text.strip():
                batch.append(src_text.strip())
            if tgt_text.strip():
                batch.append(tgt_text.strip())
            
            # æ‰¹é‡yieldï¼Œä¼˜åŒ–å†…å­˜
            if len(batch) >= batch_size:
                yield batch
                batch = []
        if batch:
            yield batch

    # 7. è®­ç»ƒåˆ†è¯å™¨ï¼ˆç”¨çœŸå®å¹³è¡Œè¯­æ–™ï¼‰
    tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
    final_vocab_size = tokenizer.get_vocab_size()
    if final_vocab_size < config.VOCAB_SIZE:
        warnings.warn(f"âš ï¸  è®­ç»ƒè¯­æ–™è¯æ±‡é‡ä¸è¶³ï¼Œå®é™…è¯æ±‡è¡¨å¤§å°ï¼ˆ{final_vocab_size}ï¼‰< é…ç½®å€¼ï¼ˆ{config.VOCAB_SIZE}ï¼‰")

    # 8. è®¾ç½®åå¤„ç†æ¨¡æ¿ï¼ˆæ·»åŠ BOS/EOSï¼Œé€‚é…æ¨¡å‹è¾“å…¥ï¼‰
    tokenizer.post_processor = TemplateProcessing(
        single="[BOS] $A [EOS]",  # å•å¥ï¼šBOS + æ–‡æœ¬ + EOS
        pair="[BOS] $A [EOS] $B [EOS]",  # æˆå¯¹æ–‡æœ¬ï¼ˆå¦‚ç¿»è¯‘ï¼‰ï¼šBOS + æºæ–‡æœ¬ + EOS + ç›®æ ‡æ–‡æœ¬ + EOS
        special_tokens=[
            ("[BOS]", tokenizer.token_to_id("[BOS]")),
            ("[EOS]", tokenizer.token_to_id("[EOS]")),
        ],
    )

    # 9. ä¿å­˜åˆ†è¯å™¨å’Œè¯æ±‡è¡¨ä¿¡æ¯
    tokenizer.save(tokenizer_path)
    vocab_info = {
        "vocab_size": final_vocab_size,
        "special_tokens": {
            "pad": tokenizer.token_to_id("[PAD]"),
            "unk": tokenizer.token_to_id("[UNK]"),
            "bos": tokenizer.token_to_id("[BOS]"),
            "eos": tokenizer.token_to_id("[EOS]")
        },
        "config_vocab_size": config.VOCAB_SIZE,
        "min_frequency": 2
    }
    with open(os.path.join(os.path.dirname(tokenizer_path), "tokenizer_info.json"), "w", encoding="utf-8") as f:
        json.dump(vocab_info, f, indent=2, ensure_ascii=False)

    print(f"ğŸ‰ åˆ†è¯å™¨è®­ç»ƒå®Œæˆï¼ä¿å­˜è·¯å¾„ï¼š{tokenizer_path}ï¼ˆè¯æ±‡è¡¨å¤§å°ï¼š{final_vocab_size}ï¼‰")
    return tokenizer

class TokenizerWrapper:
    """åˆ†è¯å™¨åŒ…è£…ç±»ï¼Œæä¾›ç»Ÿä¸€æ¥å£ï¼ˆé€‚é…è®­ç»ƒ/è¯„ä¼°è„šæœ¬ï¼‰"""
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        # ç»‘å®šç‰¹æ®Štoken IDï¼ˆç¡®ä¿å’Œæ¨¡å‹ä¸€è‡´ï¼‰
        self.pad_token_id = tokenizer.token_to_id("[PAD]")
        self.bos_token_id = tokenizer.token_to_id("[BOS]")
        self.eos_token_id = tokenizer.token_to_id("[EOS]")
        self.unk_token_id = tokenizer.token_to_id("[UNK]")
        # æ ¡éªŒç‰¹æ®Štokenæ˜¯å¦å­˜åœ¨
        assert all(id is not None for id in [self.pad_token_id, self.bos_token_id, self.eos_token_id, self.unk_token_id]), \
            "âŒ åˆ†è¯å™¨ç¼ºå°‘å¿…è¦çš„ç‰¹æ®Štokenï¼"

    def encode(self, text, max_length=None, padding=True, truncation=True):
        """ç¼–ç æ–‡æœ¬ä¸ºtoken IDsï¼ˆé€‚é…æ‰¹é‡/å•å¥è¾“å…¥ï¼‰"""
        if isinstance(text, str):
            text = [text]  # ç»Ÿä¸€ä¸ºåˆ—è¡¨æ ¼å¼
        
        # æ ¹æ®tokenizersç‰ˆæœ¬å…¼å®¹æ€§å¤„ç†å‚æ•°
        encode_kwargs = {}
        
        # æ–°ç‰ˆæœ¬tokenizersä½¿ç”¨add_special_tokensè€Œä¸æ˜¯padding/truncation
        if hasattr(self.tokenizer, 'enable_padding'):
            if padding:
                self.tokenizer.enable_padding(
                    pad_id=self.pad_token_id, 
                    pad_token="[PAD]",
                    length=max_length
                )
            else:
                self.tokenizer.no_padding()
                
            if truncation and max_length:
                self.tokenizer.enable_truncation(max_length)
            else:
                self.tokenizer.no_truncation()
        else:
            # æ—§ç‰ˆæœ¬å…¼å®¹
            encode_kwargs = {
                'padding': padding,
                'truncation': truncation,
                'max_length': max_length
            }
        
        # ç¼–ç ï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†ï¼‰
        try:
            encodings = self.tokenizer.encode_batch(text, **encode_kwargs)
        except TypeError as e:
            # å¦‚æœä»ç„¶æŠ¥é”™ï¼Œå›é€€åˆ°é€æ¡ç¼–ç 
            print(f"âš ï¸  æ‰¹é‡ç¼–ç å¤±è´¥ï¼Œä½¿ç”¨é€æ¡ç¼–ç : {e}")
            encodings = []
            for t in text:
                try:
                    encoding = self.tokenizer.encode(t, **encode_kwargs)
                    encodings.append(encoding)
                except Exception as single_e:
                    print(f"âŒ å•æ¡ç¼–ç å¤±è´¥: {single_e}")
                    # è¿”å›ç©ºç¼–ç ä½œä¸ºå…œåº•
                    empty_encoding = type('EmptyEncoding', (), {'ids': [self.pad_token_id]})()
                    encodings.append(empty_encoding)
        
        # è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼ï¼ˆè¿”å›idsåˆ—è¡¨ï¼Œé€‚é…Tensorè½¬æ¢ï¼‰
        return [encoding.ids for encoding in encodings]

    def decode(self, token_ids):
        """è§£ç token IDsä¸ºæ–‡æœ¬ï¼ˆè‡ªåŠ¨è¿‡æ»¤ç‰¹æ®Štokenï¼‰"""
        # å¤„ç†æ‰¹é‡è¾“å…¥
        if isinstance(token_ids[0], list):
            return [self._decode_single(ids) for ids in token_ids]
        return self._decode_single(token_ids)

    def _decode_single(self, token_ids):
        """è§£ç å•ä¸ªå¥å­çš„token IDs"""
        filtered_ids = [id for id in token_ids if id not in [self.pad_token_id, self.bos_token_id, self.eos_token_id]]
        return self.tokenizer.decode(filtered_ids, skip_special_tokens=False)

    def token_to_id(self, token):
        return self.tokenizer.token_to_id(token)

    def id_to_token(self, id):
        return self.tokenizer.id_to_token(id)

    def get_vocab_size(self):
        """è·å–è¯æ±‡è¡¨å¤§å°ï¼ˆè®­ç»ƒæ—¶æ›´æ–°config.VOCAB_SIZEï¼‰"""
        return self.tokenizer.get_vocab_size()

class SimpleEncoding:
    """ç®€åŒ–çš„ç¼–ç ç»“æœï¼ˆå…¼å®¹åŸæœ‰ä»£ç é€»è¾‘ï¼‰"""
    def __init__(self, ids):
        self.ids = ids