import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset, Dataset as HFDataset
import warnings

# å¿½ç•¥æ— å…³è­¦å‘Š
warnings.filterwarnings("ignore")


def load_raw_dataset(config):
    """åŠ è½½åŸå§‹å¹³è¡Œè¯­æ–™ï¼ˆæœªç¼–ç ï¼Œç”¨äºåˆ†è¯å™¨è®­ç»ƒï¼‰
    é€‚é…ä¸¤ç§æ•°æ®æºï¼š
    1. æœ¬åœ°æ•°æ®é›†ï¼ˆ{"translation": {"en": "", "de": ""}} æ ¼å¼ï¼‰
    2. Hugging Face å…¬å¼€æ•°æ®é›†
    """
    if config.USE_LOCAL_DATA:
        return _load_local_raw_dataset(config)
    else:
        return _load_hf_raw_dataset(config)


def _load_local_raw_dataset(config, is_val=False):
    """åŠ è½½æœ¬åœ°åŸå§‹æ•°æ®é›†ï¼ˆæ ¸å¿ƒé€‚é…ï¼štranslation.en/de åµŒå¥—æ ¼å¼ï¼‰"""
    # é€‰æ‹©è®­ç»ƒ/éªŒè¯æ–‡ä»¶
    filename = config.LOCAL_VAL_FILE if is_val else config.LOCAL_TRAIN_FILE
    data_path = os.path.join(config.LOCAL_DATA_PATH, filename)
    
    # æ ¡éªŒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨ï¼š{data_path}")

    raw_dataset = []  # å­˜å‚¨åŸå§‹è§£ææ•°æ®
    total_lines = 0  # åŸå§‹æ€»è¡Œæ•°
    parse_error_count = 0  # JSONè§£æé”™è¯¯æ•°
    format_error_count = 0  # å­—æ®µæ ¼å¼é”™è¯¯æ•°

    print(f"\nğŸ“¥ åŠ è½½æœ¬åœ°æ•°æ®é›†ï¼š{data_path}")
    print(f"   é€‚é…æ ¼å¼ï¼š{{'translation': {{'en': 'è‹±æ–‡', 'de': 'å¾·æ–‡'}}}}")

    with open(data_path, "r", encoding="utf-8") as f:
        file_content = f.read().strip()
        data_list = []

        # æƒ…å†µ1ï¼šæ–‡ä»¶æ˜¯å®Œæ•´çš„JSONæ•°ç»„ï¼ˆè¢« [] åŒ…è£¹ï¼‰
        if file_content.startswith("[") and file_content.endswith("]"):
            try:
                data_list = json.loads(file_content)
                total_lines = len(data_list)
                print(f"   æ£€æµ‹åˆ°JSONæ•°ç»„æ ¼å¼ï¼Œå…± {total_lines} æ¡æ•°æ®")
            except json.JSONDecodeError as e:
                raise RuntimeError(f"âŒ JSONæ•°ç»„æ ¼å¼é”™è¯¯ï¼š{str(e)}ï¼ˆè¯·æ£€æŸ¥æ‹¬å·æ˜¯å¦åŒ¹é…ã€é€—å·æ˜¯å¦å¤šä½™ï¼‰") from e
        
        # æƒ…å†µ2ï¼šæ–‡ä»¶æ˜¯æ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼ˆå«å¯èƒ½çš„é€—å·åˆ†éš”ç¬¦ï¼‰
        else:
            lines = file_content.split("\n")
            total_lines = len(lines)
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                # è·³è¿‡ç©ºè¡Œã€æ•°ç»„å…ƒç´ é—´çš„é€—å·ã€æ³¨é‡Šè¡Œ
                if not line or line.startswith(",") or line.startswith("//"):
                    continue
                try:
                    data = json.loads(line)
                    data_list.append(data)
                except json.JSONDecodeError:
                    parse_error_count += 1
                    continue

    # æå–æœ‰æ•ˆæ•°æ®ï¼ˆé€‚é… translation.en/de æ ¼å¼ï¼Œè¿‡æ»¤æ— æ•ˆæ•°æ®ï¼‰
    valid_dataset = []
    for data in data_list:
        # æ ¡éªŒæ˜¯å¦åŒ…å« translation åµŒå¥—å­—æ®µ
        if "translation" not in data or not isinstance(data["translation"], dict):
            format_error_count += 1
            continue
        
        # æå–è‹±å¾·æ–‡æœ¬ï¼ˆå»é‡ã€è¿‡æ»¤ç©ºæ–‡æœ¬ï¼‰
        en_text = data["translation"].get("en", "").strip()
        de_text = data["translation"].get("de", "").strip()
        
        # è¿‡æ»¤æ¡ä»¶ï¼šæ–‡æœ¬éç©º + é•¿åº¦åœ¨åˆç†èŒƒå›´ï¼ˆ2~120å­—ç¬¦ï¼‰
        if not (en_text and de_text):
            format_error_count += 1
            continue
        if len(en_text) < 2 or len(en_text) > 120 or len(de_text) < 2 or len(de_text) > 120:
            format_error_count += 1
            continue
        
        # ä¿ç•™åŸå§‹æ ¼å¼ï¼ŒåŒæ—¶æ–°å¢å¹³çº§å­—æ®µï¼ˆå…¼å®¹åˆ†è¯å™¨å’Œæ¨¡å‹ï¼‰
        valid_dataset.append({
            "translation": {"en": en_text, "de": de_text},
            "source": en_text if config.SRC_LANG == "en" else de_text,
            "target": de_text if config.TGT_LANG == "de" else en_text
        })

    # æˆªå–æ ·æœ¬æ•°é‡ï¼ˆæŒ‰é…ç½®çš„è®­ç»ƒ/éªŒè¯é›†å¤§å°ï¼‰
    max_sample_size = config.VAL_SIZE if is_val else config.TRAIN_SIZE
    if len(valid_dataset) > max_sample_size:
        valid_dataset = valid_dataset[:max_sample_size]
        print(f"   æ•°æ®é›†è¿‡å¤§ï¼Œæˆªå–å‰ {max_sample_size} æ¡æœ‰æ•ˆæ ·æœ¬")

    # è¾“å‡ºåŠ è½½ç»Ÿè®¡
    print(f"âœ… æœ¬åœ°æ•°æ®é›†åŠ è½½å®Œæˆï¼š")
    print(f"   - åŸå§‹æ•°æ®ï¼š{total_lines} æ¡")
    print(f"   - æœ‰æ•ˆæ•°æ®ï¼š{len(valid_dataset)} æ¡")
    print(f"   - è§£æé”™è¯¯ï¼š{parse_error_count} æ¡ï¼ˆJSONæ ¼å¼é”™è¯¯ï¼‰")
    print(f"   - æ ¼å¼é”™è¯¯ï¼š{format_error_count} æ¡ï¼ˆç¼ºå°‘å­—æ®µ/ç©ºæ–‡æœ¬/é•¿åº¦å¼‚å¸¸ï¼‰")

    return valid_dataset


def _load_hf_raw_dataset(config):
    """åŠ è½½Hugging Faceå…¬å¼€åŸå§‹æ•°æ®é›†ï¼ˆç”¨äºåˆ†è¯å™¨è®­ç»ƒï¼‰"""
    print(f"\nğŸ“¥ åŠ è½½å…¬å¼€æ•°æ®é›†ï¼š{config.DATASET_NAME}-{config.DATASET_CONFIG}")
    try:
        # åŠ è½½è®­ç»ƒé›†å…¨é‡æ•°æ®ï¼ˆä¿è¯åˆ†è¯å™¨è¯­æ–™è¦†ç›–åº¦ï¼‰
        dataset = load_dataset(
            config.DATASET_NAME,
            config.DATASET_CONFIG,
            split="train",
            trust_remote_code=True,
            cache_dir=os.path.join("data", "cache")  # ç¼“å­˜è·¯å¾„ï¼Œé¿å…é‡å¤ä¸‹è½½
        )
    except Exception as e:
        raise RuntimeError(f"âŒ å…¬å¼€æ•°æ®é›†åŠ è½½å¤±è´¥ï¼š{str(e)}ï¼ˆè¯·æ£€æŸ¥æ•°æ®é›†åç§°/é…ç½®æ˜¯å¦æ­£ç¡®ï¼‰") from e

    # æˆªå–æ ·æœ¬ï¼ˆåŠ é€Ÿåˆ†è¯å™¨è®­ç»ƒï¼‰
    max_sample_size = min(len(dataset), config.TRAIN_SIZE * 2)  # å–2å€è®­ç»ƒé›†å¤§å°
    dataset = dataset.select(range(max_sample_size))

    print(f"âœ… å…¬å¼€æ•°æ®é›†åŠ è½½å®Œæˆï¼š{len(dataset)} æ¡æ ·æœ¬")
    return dataset


class TranslationDataset(Dataset):
    """ç¿»è¯‘æ•°æ®é›†ç±»ï¼ˆé€‚é…ä¸¤ç§æ ¼å¼ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒ/è¯„ä¼°ï¼‰"""
    def __init__(self, raw_data, tokenizer, config, is_train=True):
        self.raw_data = raw_data
        self.tokenizer = tokenizer
        self.max_length = config.MAX_LENGTH
        self.src_lang = config.SRC_LANG
        self.tgt_lang = config.TGT_LANG
        self.is_train = is_train

    def __len__(self):
        return len(self.raw_data)

    def __getitem__(self, idx):
        data = self.raw_data[idx]

        # æå–æºæ–‡æœ¬å’Œç›®æ ‡æ–‡æœ¬ï¼ˆé€‚é…ä¸¤ç§æ ¼å¼ï¼‰
        if "translation" in data:
            # æ ¼å¼1ï¼š{"translation": {"en": "", "de": ""}}ï¼ˆæœ¬åœ°æ•°æ®é›†ï¼‰
            src_text = data["translation"][self.src_lang].strip()
            tgt_text = data["translation"][self.tgt_lang].strip()
        elif "source" in data and "target" in data:
            # æ ¼å¼2ï¼š{"source": "", "target": ""}ï¼ˆå…¼å®¹å…¬å¼€æ•°æ®é›†ï¼‰
            src_text = data["source"].strip()
            tgt_text = data["target"].strip()
        else:
            src_text = ""
            tgt_text = ""

        # ç¼–ç æ–‡æœ¬ï¼ˆé€‚é…åˆ†è¯å™¨çš„encodeæ¥å£ï¼‰
        src_ids = self.tokenizer.encode(
            src_text,
            max_length=self.max_length,
            padding=False,
            truncation=True
        )[0]  # encodeè¿”å›åˆ—è¡¨ï¼Œå–å•å¥ç»“æœ
        tgt_ids = self.tokenizer.encode(
            tgt_text,
            max_length=self.max_length,
            padding=False,
            truncation=True
        )[0]

        # è¿”å›æ¨¡å‹æ‰€éœ€çš„tensoræ ¼å¼
        return {
            "src_ids": torch.tensor(src_ids, dtype=torch.long),
            "tgt_ids": torch.tensor(tgt_ids, dtype=torch.long),
            "src_text": src_text,
            "tgt_text": tgt_text
        }


def collate_fn(batch, pad_token_id):
    """æ‰¹é‡å¤„ç†å‡½æ•°ï¼ˆpaddingåˆ°æ‰¹æ¬¡å†…æœ€å¤§é•¿åº¦ï¼Œé€‚é…æ¨¡å‹è¾“å…¥ï¼‰"""
    # æå–æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰å­—æ®µ
    src_ids = [item["src_ids"] for item in batch]
    tgt_ids = [item["tgt_ids"] for item in batch]
    src_texts = [item["src_text"] for item in batch]
    tgt_texts = [item["tgt_text"] for item in batch]

    # å¯¹sourceå’Œtargetè¿›è¡Œpaddingï¼ˆç”¨pad_token_idå¡«å……ï¼‰
    src_ids_padded = torch.nn.utils.rnn.pad_sequence(
        src_ids, batch_first=True, padding_value=pad_token_id
    )
    tgt_ids_padded = torch.nn.utils.rnn.pad_sequence(
        tgt_ids, batch_first=True, padding_value=pad_token_id
    )

    return {
        "src_ids": src_ids_padded,
        "tgt_ids": tgt_ids_padded,
        "src_texts": src_texts,
        "tgt_texts": tgt_texts
    }


def get_data_loaders(config, tokenizer):
    """è·å–è®­ç»ƒ/éªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰"""
    print("\nğŸ“¥ æ„å»ºæ•°æ®åŠ è½½å™¨...")
    # åŠ è½½åŸå§‹æ•°æ®ï¼ˆæ ¹æ®é…ç½®é€‰æ‹©æœ¬åœ°/å…¬å¼€æ•°æ®é›†ï¼‰
    if config.USE_LOCAL_DATA:
        train_raw = _load_local_raw_dataset(config, is_val=False)
        val_raw = _load_local_raw_dataset(config, is_val=True)
    else:
        train_raw, val_raw = _load_hf_encoded_dataset(config)

    # æ„å»ºç¼–ç åçš„æ•°æ®é›†ï¼ˆé€‚é…æ¨¡å‹è¾“å…¥ï¼‰
    train_dataset = TranslationDataset(train_raw, tokenizer, config, is_train=True)
    val_dataset = TranslationDataset(val_raw, tokenizer, config, is_train=False)

    # æ„å»ºDataLoaderï¼ˆæ‰¹é‡åŠ è½½+å¤šçº¿ç¨‹é¢„å¤„ç†ï¼‰
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,  # è®­ç»ƒé›†æ‰“ä¹±
        num_workers=config.NUM_WORKERS,
        collate_fn=lambda x: collate_fn(x, tokenizer.pad_token_id),
        pin_memory=True,  # åŠ é€ŸGPUæ•°æ®ä¼ è¾“
        drop_last=True  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´æ‰¹æ¬¡
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE * 2,  # éªŒè¯é›†æ‰¹æ¬¡åŠ å€ï¼ŒåŠ é€Ÿè¯„ä¼°
        shuffle=False,  # éªŒè¯é›†ä¸æ‰“ä¹±
        num_workers=config.NUM_WORKERS,
        collate_fn=lambda x: collate_fn(x, tokenizer.pad_token_id),
        pin_memory=True
    )

    # è¾“å‡ºæ•°æ®åŠ è½½å™¨ç»Ÿè®¡
    print(f"âœ… æ•°æ®åŠ è½½å™¨æ„å»ºå®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_dataset)} æ¡æ ·æœ¬ï¼Œ{len(train_loader)} ä¸ªæ‰¹æ¬¡")
    print(f"   - éªŒè¯é›†ï¼š{len(val_dataset)} æ¡æ ·æœ¬ï¼Œ{len(val_loader)} ä¸ªæ‰¹æ¬¡")
    print(f"   - æ‰¹æ¬¡å¤§å°ï¼šè®­ç»ƒé›† {config.BATCH_SIZE}ï¼ŒéªŒè¯é›† {config.BATCH_SIZE * 2}")

    return train_loader, val_loader


def _load_hf_encoded_dataset(config):
    """åŠ è½½Hugging Faceå…¬å¼€æ•°æ®é›†ï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒ/éªŒè¯ï¼‰"""
    print(f"\nğŸ“¥ åŠ è½½å…¬å¼€è®­ç»ƒ/éªŒè¯æ•°æ®é›†ï¼š{config.DATASET_NAME}-{config.DATASET_CONFIG}")
    try:
        dataset = load_dataset(
            config.DATASET_NAME,
            config.DATASET_CONFIG,
            splits=["train", "validation"],
            trust_remote_code=True,
            cache_dir=os.path.join("data", "cache")
        )
    except Exception as e:
        raise RuntimeError(f"âŒ å…¬å¼€æ•°æ®é›†åŠ è½½å¤±è´¥ï¼š{str(e)}") from e

    # æˆªå–æ ·æœ¬ï¼ˆæ§åˆ¶æ•°æ®è§„æ¨¡ï¼ŒåŠ é€Ÿè®­ç»ƒï¼‰
    train_raw = dataset["train"].select(range(min(len(dataset["train"]), config.TRAIN_SIZE)))
    val_raw = dataset["validation"].select(range(min(len(dataset["validation"]), config.VAL_SIZE)))

    return train_raw, val_raw